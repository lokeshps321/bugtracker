"""
MLOps Testing Script
Verifies that PM corrections actually improve model predictions
"""

import requests
import json
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
logger = logging.getLogger(__name__)

API_URL = "http://localhost:8000"

def test_mlops_feedback_loop():
    logger.info("=" * 70)
    logger.info("MLOPS TESTING: Verifying Feedback Loop Works")
    logger.info("=" * 70)
    
    # Test cases: descriptions where we'll correct predictions
    test_cases = [
        {
            "description": "Docker container crashes when deploying to production kubernetes cluster",
            "project": "Infrastructure",
            "expected_team_before": "Backend",  # Old model likely predicts this
            "correct_team": "DevOps",  # PM correction
            "correct_severity": "high"
        },
        {
            "description": "iOS app freezes when user tries to take a photo in profile settings",
            "project": "MobileApp",
            "expected_team_before": "Backend",  # Old model likely predicts this
            "correct_team": "Mobile",  # PM correction
            "correct_severity": "medium"
        },
        {
            "description": "Login button has wrong color on the dashboard",
            "project": "WebApp",
            "expected_team_before": "Backend",  # Old model likely predicts this
            "correct_team": "Frontend",  # PM correction
            "correct_severity": "low"
        },
    ]
    
    logger.info(f"\nTesting {len(test_cases)} correction scenarios...\n")
    
    for i, test in enumerate(test_cases, 1):
        logger.info(f"Test Case {i}: {test['description'][:60]}...")
        logger.info("-" * 70)
        
        # Step 1: Get initial prediction
        logger.info("Step 1: Getting initial prediction from current model...")
        resp = requests.post(
            f"{API_URL}/predict",
            json={"description": test['description'], "project": test['project']},
            timeout=10
        )
        
        if resp.status_code != 200:
            logger.error(f"‚ùå Prediction failed: {resp.status_code}")
            continue
        
        initial_pred = resp.json()
        logger.info(f"  Severity: {initial_pred['severity']}")
        logger.info(f"  Team: {initial_pred['team']}")
        
        # Step 2: Report bug with predictions
        logger.info("\nStep 2: Reporting bug with AI predictions...")
        bug_resp = requests.post(
            f"{API_URL}/report_bug",
            json={
                "description": test['description'],
                "project": test['project'],
                "severity": initial_pred['severity'],
                "team": initial_pred['team']
            },
            headers={"Authorization": "Bearer fake_pm_token"},  # Mock auth
            timeout=10
        )
        
        if bug_resp.status_code not in [200, 201]:
            logger.warning(f"  Bug report returned {bug_resp.status_code}, continuing...")
            bug_id = 999 + i  # Mock ID
        else:
            bug_data = bug_resp.json()
            bug_id = bug_data.get('id', 999 + i)
            logger.info(f"  ‚úÖ Bug #{bug_id} reported")
        
        # Step 3: PM corrects the prediction
        logger.info(f"\nStep 3: PM corrects Bug #{bug_id}...")
        logger.info(f"  Correcting team: {initial_pred['team']} ‚Üí {test['correct_team']}")
        logger.info(f"  Correcting severity: {initial_pred['severity']} ‚Üí {test['correct_severity']}")
        
        correction_resp = requests.post(
            f"{API_URL}/update_bug",
            json={
                "bug_id": bug_id,
                "status": "open",  # Keep status
                "correction_severity": test['correct_severity'],
                "correction_team": test['correct_team']
            },
            headers={"Authorization": "Bearer fake_pm_token"},
            timeout=10
        )
        
        if correction_resp.status_code == 200:
            msg = correction_resp.json().get('message', '')
            logger.info(f"  ‚úÖ Correction saved")
            if "Feedback Recorded" in msg or "retraining" in msg.lower():
                logger.info(f"  ‚úÖ MLOps: Feedback recorded for model improvement")
        else:
            logger.warning(f"  ‚ö†Ô∏è  Correction returned {correction_resp.status_code}")
        
        # Step 4: Verify prediction improves (if model retrains immediately)
        logger.info(f"\nStep 4: Re-predicting same bug...")
        resp2 = requests.post(
            f"{API_URL}/predict",
            json={"description": test['description'], "project": test['project']},
            timeout=10
        )
        
        if resp2.status_code == 200:
            new_pred = resp2.json()
            logger.info(f"  New Severity: {new_pred['severity']}")
            logger.info(f"  New Team: {new_pred['team']}")
            
            # Check if improved
            team_improved = (new_pred['team'] == test['correct_team'])
            severity_improved = (new_pred['severity'] == test['correct_severity'])
            
            if team_improved and severity_improved:
                logger.info(f"  üéØ PERFECT! Model now predicts corrections accurately!")
            elif team_improved:
                logger.info(f"  ‚úÖ Team prediction improved (correct now)")
            elif severity_improved:
                logger.info(f"  ‚úÖ Severity prediction improved (correct now)")
            else:
                logger.info(f"  ‚ÑπÔ∏è  Prediction not yet improved (may need more feedback data)")
        
        logger.info("\n" + "=" * 70 + "\n")
    
    logger.info("=" * 70)
    logger.info("MLOPS TEST COMPLETE")
    logger.info("=" * 70)
    logger.info("\nSummary:")
    logger.info("- All test corrections were saved to the database")
    logger.info("- These corrections will be used in next model retraining")
    logger.info("- To see improvements, retrain models on collected feedback\n")

if __name__ == "__main__":
    try:
        test_mlops_feedback_loop()
    except Exception as e:
        logger.error(f"Test failed: {str(e)}")
        raise
